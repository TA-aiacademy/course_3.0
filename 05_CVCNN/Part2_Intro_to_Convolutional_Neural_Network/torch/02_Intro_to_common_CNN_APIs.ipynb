{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f901de8a",
      "metadata": {
        "tags": [],
        "id": "f901de8a"
      },
      "source": [
        "# **Intro to common CNN APIs**\n",
        "此份程式碼會介紹在 CNN model 當中常使用的 Layers。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c68f4a3",
      "metadata": {
        "tags": [],
        "id": "3c68f4a3"
      },
      "source": [
        "## 本章節大綱\n",
        "* [Conv2D( filters, kernel_size, strides, use_bias)](#Conv2D)\n",
        "  * [use_bias](#use-bias)\n",
        "  * [Multi-Channels](#Multi-Channels-with-1-Filter)\n",
        "  * [filters](#filters)\n",
        "  * [kernel_size](#kernel-_-size)\n",
        "  * [strides](#strides)\n",
        "* [Flatten](#Flatten)\n",
        "* [Padding](#Padding)\n",
        "* [Pooling](#Pooling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80560b10",
      "metadata": {
        "id": "80560b10"
      },
      "outputs": [],
      "source": [
        "# 下載課程所需檔案\n",
        "!wget -q \"https://github.com/TA-aiacademy/course_3.0/releases/download/CVCNN_Data/cnn_part2_data.zip\"\n",
        "!unzip -q cnn_part2_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa8a9dfa",
      "metadata": {
        "id": "aa8a9dfa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d8b5df",
      "metadata": {
        "id": "c0d8b5df"
      },
      "outputs": [],
      "source": [
        "input_img = torch.tensor(\n",
        "    [[0, 0, 0, 0, 0, 0],\n",
        "     [0, 0, 0, 1, 1, 0],\n",
        "     [0, 1, 1, 1, 1, 0],\n",
        "     [0, 0, 1, 0, 1, 0],\n",
        "     [0, 0, 0, 1, 0, 0],\n",
        "     [0, 0, 0, 0, 0, 0]],\n",
        "    dtype=torch.float32\n",
        ")\n",
        "input_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b77681b2",
      "metadata": {
        "id": "b77681b2"
      },
      "outputs": [],
      "source": [
        "# add batch and channel dim\n",
        "input_img = input_img.unsqueeze(0).unsqueeze(0)\n",
        "print(input_img.shape)\n",
        "print(\"(batch_size, channels, height, width)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72451b40",
      "metadata": {
        "id": "72451b40"
      },
      "source": [
        "* ## Conv2D\n",
        "![conv2D](https://hackmd.io/_uploads/Hy6RbRUIp.gif)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = nn.Conv2d(1, 1, kernel_size=3, bias=False)\n",
        "layer.weight.shape\n",
        "# weight shape: (out_channels, in_channels ,H, W)"
      ],
      "metadata": {
        "id": "x-Pk8TBTssv1"
      },
      "id": "x-Pk8TBTssv1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_init = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # set kernel weight\n",
        "    weights = filter_init.reshape(1, 1, 3, 3)\n",
        "    layer.weight = nn.Parameter(weights)\n",
        "\n",
        "\n",
        "conv_result = layer(input_img)\n",
        "print(conv_result)\n",
        "print(conv_result.shape)"
      ],
      "metadata": {
        "id": "LylmEvQsoA9j"
      },
      "id": "LylmEvQsoA9j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5cb815bc",
      "metadata": {
        "id": "5cb815bc"
      },
      "source": [
        "[(back...)](#Convolution2D)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2835a71c",
      "metadata": {
        "id": "2835a71c"
      },
      "source": [
        "* ## use bias\n",
        "![use bias](https://hackmd.io/_uploads/BkFRfR8La.gif)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = nn.Conv2d(1, 1, kernel_size=3, bias=True)\n",
        "# bias shape: (out_channels, )\n",
        "layer.bias, layer.bias.shape"
      ],
      "metadata": {
        "id": "aZFB1yhSsfWF"
      },
      "id": "aZFB1yhSsfWF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias_init = torch.ones(1, ).to(torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    weights = filter_init.reshape(1, 1, 3, 3)\n",
        "    layer.weight = nn.Parameter(weights)\n",
        "    # set kernel bias\n",
        "    layer.bias = nn.Parameter(bias_init)\n",
        "\n",
        "\n",
        "bias_result = layer(input_img)\n",
        "print(bias_result)\n",
        "print(bias_result.shape)"
      ],
      "metadata": {
        "id": "29VbEUytsz0g"
      },
      "id": "29VbEUytsz0g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ec6d7f5",
      "metadata": {
        "id": "9ec6d7f5"
      },
      "outputs": [],
      "source": [
        "print(bias_result.shape)\n",
        "print(bias_result.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3c1ad5e",
      "metadata": {
        "id": "b3c1ad5e"
      },
      "source": [
        "[(back...)](#Convolution2D)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f310c127",
      "metadata": {
        "id": "f310c127"
      },
      "source": [
        "* ## Multi Channels with 1 Filter\n",
        "![Multi Channels with 1 filter](https://hackmd.io/_uploads/S1q1m08I6.gif)\n",
        "![QEjI0jq](https://hackmd.io/_uploads/By9e70I8a.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c27e1be",
      "metadata": {
        "id": "8c27e1be"
      },
      "outputs": [],
      "source": [
        "input_img = np.load(\"./data/conv2d_multichannel_input.npy\")\n",
        "print(input_img.shape)\n",
        "print(input_img.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "244d32be",
      "metadata": {
        "id": "244d32be"
      },
      "outputs": [],
      "source": [
        "input_img = input_img[np.newaxis, ...]\n",
        "print(input_img.shape)\n",
        "print(\"(Batch_size, Height, Width, Channel)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d47433",
      "metadata": {
        "id": "b4d47433"
      },
      "outputs": [],
      "source": [
        "input_img = torch.tensor(input_img, dtype=torch.float32)\n",
        "input_img = input_img.permute(0, 3, 1, 2) # to channel first\n",
        "print(input_img.shape, input_img.dtype)\n",
        "print(\"(Batch_size, Channel, Height, Width)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2a196ac",
      "metadata": {
        "id": "a2a196ac"
      },
      "outputs": [],
      "source": [
        "filter_init = np.load(\"./data/conv2d_multichannelfilter.npy\")\n",
        "print(filter_init.shape)\n",
        "print(\"(Height, Width, Channel, Num of Filters)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46366f3c",
      "metadata": {
        "id": "46366f3c"
      },
      "outputs": [],
      "source": [
        "# weight shape: (out_channels, in_channels ,H, W)\n",
        "kernel_init = torch.tensor(filter_init, dtype=torch.float32).permute(3, 2, 0, 1)\n",
        "print(kernel_init.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97b20b9c",
      "metadata": {
        "id": "97b20b9c"
      },
      "outputs": [],
      "source": [
        "multichannel_layer = nn.Conv2d(3, 1, kernel_size=3)\n",
        "with torch.no_grad():\n",
        "    # set kernel weight\n",
        "    multichannel_layer.weight = nn.Parameter(kernel_init)\n",
        "    # set kernel bias\n",
        "    multichannel_layer.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "multichannel_result = multichannel_layer(input_img)\n",
        "print(multichannel_result.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b35339aa",
      "metadata": {
        "id": "b35339aa"
      },
      "outputs": [],
      "source": [
        "print(multichannel_result.shape)\n",
        "print(multichannel_result.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe90f00d",
      "metadata": {
        "id": "fe90f00d"
      },
      "source": [
        "[(back...)](#Convolution2D)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0972b87a",
      "metadata": {
        "id": "0972b87a"
      },
      "source": [
        "* ## filters\n",
        "![filters](https://hackmd.io/_uploads/BJV77RUU6.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d4e324c",
      "metadata": {
        "id": "8d4e324c"
      },
      "outputs": [],
      "source": [
        "multi_filter_init = np.zeros((3, 3, 3, 8))\n",
        "for i in range(8):\n",
        "    multi_filter_init[:, :, :, i] = filter_init.squeeze()\n",
        "multi_filter_init = multi_filter_init.astype('float32')\n",
        "\n",
        "print(multi_filter_init.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "241eef6c",
      "metadata": {
        "id": "241eef6c"
      },
      "outputs": [],
      "source": [
        "kernel_init = torch.tensor(multi_filter_init)\n",
        "kernel_init = kernel_init.permute(3, 2, 0, 1)\n",
        "print(kernel_init.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a03eca4",
      "metadata": {
        "id": "7a03eca4"
      },
      "outputs": [],
      "source": [
        "multifilter = nn.Conv2d(3, 8, kernel_size=3)\n",
        "with torch.no_grad():\n",
        "    # set kernel weight\n",
        "    multifilter.weight = nn.Parameter(kernel_init)\n",
        "    # set kernel bias\n",
        "    multifilter.bias = nn.Parameter(torch.zeros(8))\n",
        "\n",
        "    multifilter_result = multifilter(input_img)\n",
        "    print(multifilter_result.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6af04c5",
      "metadata": {
        "id": "e6af04c5"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(8):\n",
        "    plt.subplot(2, 4, i+1)\n",
        "    plt.imshow(multifilter_result.squeeze()[i, :, :])\n",
        "plt.show()\n",
        "\n",
        "print(multifilter_result.shape)\n",
        "print(multifilter_result.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f352ce5e",
      "metadata": {
        "id": "f352ce5e"
      },
      "source": [
        "[(back...)](#Convolution2D)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59caf228",
      "metadata": {
        "id": "59caf228"
      },
      "source": [
        "* ## strides\n",
        "![strides](https://hackmd.io/_uploads/r17N708Up.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58221fbd",
      "metadata": {
        "id": "58221fbd"
      },
      "outputs": [],
      "source": [
        "input_img = np.load(\"./data/conv2d_1channel_input.npy\")\n",
        "input_img = torch.tensor(input_img).permute(0, 3, 1, 2)\n",
        "input_img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8bd52a9",
      "metadata": {
        "id": "f8bd52a9"
      },
      "outputs": [],
      "source": [
        "filter_init = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype='float32')\n",
        "filter_init = torch.tensor(filter_init).unsqueeze(0).unsqueeze(0)\n",
        "print(filter_init.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b73eaf",
      "metadata": {
        "id": "b6b73eaf"
      },
      "outputs": [],
      "source": [
        "conv_stride_layer = nn.Conv2d(1, 1, kernel_size=3, stride=(2, 2))\n",
        "with torch.no_grad():\n",
        "    # set kernel weight\n",
        "    conv_stride_layer.weight = nn.Parameter(filter_init)\n",
        "    # set kernel bias\n",
        "    conv_stride_layer.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    stride_result = conv_stride_layer(input_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76bf1900",
      "metadata": {
        "id": "76bf1900"
      },
      "outputs": [],
      "source": [
        "print(stride_result.shape)\n",
        "print(stride_result.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2fb2f7a",
      "metadata": {
        "id": "b2fb2f7a"
      },
      "source": [
        "![image](https://hackmd.io/_uploads/Sk__X0LIa.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53bb569f",
      "metadata": {
        "id": "53bb569f"
      },
      "source": [
        "# Flatten\n",
        "\n",
        "* [Way1-Reshape](#Way1---Reshape)\n",
        "* [Way2-Flatten](#Way2---Flatten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b1e7030",
      "metadata": {
        "id": "2b1e7030"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8452c292",
      "metadata": {
        "id": "8452c292"
      },
      "outputs": [],
      "source": [
        "input_img1 = np.array([[0, 1, 2, 3],\n",
        "                       [4, 5, 6, 7],\n",
        "                       [8, 9, 10, 11],\n",
        "                       [12, 13, 14, 15]], dtype='float32')\n",
        "input_img1 = input_img1[np.newaxis, np.newaxis, ...]\n",
        "input_img1 = torch.tensor(input_img1, dtype=torch.float32)\n",
        "print(input_img1.shape)\n",
        "\n",
        "# repeat input_img to 8 channels\n",
        "input_img2 = input_img1.repeat(1, 8, 1, 1)\n",
        "print(input_img2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d98b7a4",
      "metadata": {
        "id": "4d98b7a4"
      },
      "source": [
        "* ## Way1 - Reshape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "413e8f54",
      "metadata": {
        "id": "413e8f54"
      },
      "outputs": [],
      "source": [
        "reshape_result = input_img1.reshape(input_img1.shape[0], -1)\n",
        "print(input_img1.shape)\n",
        "print(reshape_result.shape)\n",
        "print(reshape_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af18f01",
      "metadata": {
        "id": "2af18f01"
      },
      "outputs": [],
      "source": [
        "reshape_result = input_img2.reshape(input_img2.shape[0], -1)\n",
        "print(input_img2.shape)\n",
        "print(reshape_result.shape)\n",
        "print(reshape_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8940922",
      "metadata": {
        "id": "e8940922"
      },
      "source": [
        "[(back...)](#Flatten)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22fab2e5",
      "metadata": {
        "id": "22fab2e5"
      },
      "source": [
        "* ## Way2 - Flatten"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e4351c",
      "metadata": {
        "id": "b2e4351c"
      },
      "source": [
        "![Flatten](https://hackmd.io/_uploads/ByBFmR8Ia.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6ff0d7",
      "metadata": {
        "id": "6b6ff0d7"
      },
      "outputs": [],
      "source": [
        "\n",
        "flatten_result = nn.Flatten()(input_img1)\n",
        "flatten_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "453d3c9b",
      "metadata": {
        "id": "453d3c9b"
      },
      "outputs": [],
      "source": [
        "print(input_img1.shape)\n",
        "print(flatten_result.shape)\n",
        "print(flatten_result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flatten_result = nn.Flatten()(input_img2)\n",
        "flatten_result"
      ],
      "metadata": {
        "id": "PaWn4KQAARSR"
      },
      "id": "PaWn4KQAARSR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_img2.shape)\n",
        "print(flatten_result.shape)\n",
        "print(flatten_result)"
      ],
      "metadata": {
        "id": "vwLGr47-AVFg"
      },
      "id": "vwLGr47-AVFg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "77cbaa74",
      "metadata": {
        "id": "77cbaa74"
      },
      "source": [
        "![Flatten_M](https://hackmd.io/_uploads/HyZ5Q08Ia.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9858fdcd",
      "metadata": {
        "id": "9858fdcd"
      },
      "source": [
        "[(back...)](#Flatten)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "006f9bee",
      "metadata": {
        "id": "006f9bee"
      },
      "source": [
        "# Padding\n",
        "\n",
        "* [padding='valid'](#padding='VALID')\n",
        "* [padding='same'](#padding='SAME')\n",
        "* [padding=INT](#padding=INT)\n",
        "* [nn.ZeroPad2d](#nn.ZeroPad2d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "314c338d",
      "metadata": {
        "id": "314c338d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6b923f7",
      "metadata": {
        "id": "c6b923f7"
      },
      "outputs": [],
      "source": [
        "input_img = np.array([[0, 0, 0, 0, 0, 0],\n",
        "                      [0, 0, 0, 1, 1, 0],\n",
        "                      [0, 1, 1, 1, 1, 0],\n",
        "                      [0, 0, 1, 0, 1, 0],\n",
        "                      [0, 0, 0, 1, 0, 0],\n",
        "                      [0, 0, 0, 0, 0, 0]], dtype='float32')\n",
        "input_img = input_img[np.newaxis, np.newaxis, ...]\n",
        "input_img = torch.tensor(input_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "822f3f54",
      "metadata": {
        "id": "822f3f54"
      },
      "outputs": [],
      "source": [
        "def kernel_init(layer, output_channel=1):\n",
        "    filter_init = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
        "    filter_init = torch.tensor(filter_init).float().unsqueeze(0).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        filter_init = nn.Parameter(filter_init)\n",
        "        layer.weight = filter_init\n",
        "        layer.bias = nn.Parameter(torch.zeros(output_channel))\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0cee328",
      "metadata": {
        "id": "a0cee328"
      },
      "source": [
        "* ## padding='VALID'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d300c4f",
      "metadata": {
        "id": "6d300c4f"
      },
      "outputs": [],
      "source": [
        "layer = nn.Conv2d(1, 1, kernel_size=3, padding='valid')\n",
        "kernel_init(layer)\n",
        "nopad_result = layer(input_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fa5ba29",
      "metadata": {
        "id": "4fa5ba29"
      },
      "outputs": [],
      "source": [
        "print(input_img.shape)\n",
        "print(nopad_result.shape)\n",
        "print(nopad_result.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5e2dcd3",
      "metadata": {
        "id": "b5e2dcd3"
      },
      "source": [
        "[(back...)](#Padding)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7cd777c",
      "metadata": {
        "id": "a7cd777c"
      },
      "source": [
        "* ## padding='SAME'\n",
        "![padding_s](https://hackmd.io/_uploads/HyQBZ1PUp.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5b6f9e2",
      "metadata": {
        "id": "f5b6f9e2"
      },
      "outputs": [],
      "source": [
        "layer = nn.Conv2d(1, 1, kernel_size=3, padding='same')\n",
        "kernel_init(layer)\n",
        "pad_result = layer(input_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf8b2f88",
      "metadata": {
        "id": "bf8b2f88"
      },
      "outputs": [],
      "source": [
        "print(input_img.shape)\n",
        "print(pad_result.shape)\n",
        "print(pad_result.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68aed2e9",
      "metadata": {
        "id": "68aed2e9"
      },
      "source": [
        "[(back...)](#Padding)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38cd7b04",
      "metadata": {
        "id": "38cd7b04"
      },
      "source": [
        "## nn.ZeroPad2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eea96dd0",
      "metadata": {
        "id": "eea96dd0"
      },
      "outputs": [],
      "source": [
        "zero_padding = nn.ZeroPad2d(padding=1)(input_img)\n",
        "layer = nn.Conv2d(1, 1, kernel_size=3)\n",
        "kernel_init(layer)\n",
        "zero_result = layer(zero_padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce0d4cc",
      "metadata": {
        "id": "0ce0d4cc"
      },
      "outputs": [],
      "source": [
        "print(input_img.shape)\n",
        "print(zero_padding.shape)\n",
        "print(zero_padding.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bcec283",
      "metadata": {
        "id": "1bcec283"
      },
      "outputs": [],
      "source": [
        "print(zero_result.shape)\n",
        "print(zero_result.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb67bcf3",
      "metadata": {
        "id": "bb67bcf3"
      },
      "source": [
        "[(back...)](#Padding)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e738bd8a",
      "metadata": {
        "id": "e738bd8a"
      },
      "source": [
        "# Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a65bc0d",
      "metadata": {
        "id": "2a65bc0d"
      },
      "source": [
        "\n",
        "* [Average Pooling](#Average-Pooling)\n",
        "* [Max Pooling](#Max-Pooling)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8d4d7e",
      "metadata": {
        "id": "2e8d4d7e"
      },
      "source": [
        "![image](https://hackmd.io/_uploads/H1LFb1D8p.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa850e07",
      "metadata": {
        "id": "aa850e07"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7c9588",
      "metadata": {
        "id": "6d7c9588"
      },
      "outputs": [],
      "source": [
        "input_img = np.array([[1, 2, 2, 0],\n",
        "                      [1, 2, 3, 2],\n",
        "                      [3, 1, 3, 2],\n",
        "                      [0, 2, 0, 2]], dtype='float32')\n",
        "input_img = torch.tensor(input_img).unsqueeze(0).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac25265a",
      "metadata": {
        "id": "ac25265a"
      },
      "source": [
        "* ## Average Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df82163b",
      "metadata": {
        "id": "df82163b"
      },
      "source": [
        "![avg pool](https://hackmd.io/_uploads/HkgoW1v86.gif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f802337",
      "metadata": {
        "id": "2f802337"
      },
      "outputs": [],
      "source": [
        "avg_result = nn.AvgPool2d(kernel_size=2)(input_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2844449",
      "metadata": {
        "id": "a2844449"
      },
      "outputs": [],
      "source": [
        "print(input_img.shape)\n",
        "print(avg_result.shape)\n",
        "print(avg_result.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9482529c",
      "metadata": {
        "id": "9482529c"
      },
      "source": [
        "[(back...)](#Pooling)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "485dd32f",
      "metadata": {
        "id": "485dd32f"
      },
      "source": [
        "* ## Max Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47b64082",
      "metadata": {
        "id": "47b64082"
      },
      "source": [
        "![max pool](https://hackmd.io/_uploads/rkCob1P8p.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e89829",
      "metadata": {
        "id": "29e89829"
      },
      "outputs": [],
      "source": [
        "max_result = nn.MaxPool2d(kernel_size=2)(input_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98236dcd",
      "metadata": {
        "id": "98236dcd"
      },
      "outputs": [],
      "source": [
        "print(input_img.shape)\n",
        "print(max_result.shape)\n",
        "print(max_result.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac8ed073",
      "metadata": {
        "id": "ac8ed073"
      },
      "source": [
        "[(back...)](#Pooling)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07dc5b9e",
      "metadata": {
        "id": "07dc5b9e"
      },
      "source": [
        "# GlobalPooling\n",
        "\n",
        "* [Global Average Pooling](#Global-Average-Pooling)\n",
        "* [Global Max Pooling](#Global-Max-Pooling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8607fb7",
      "metadata": {
        "id": "d8607fb7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86da18d",
      "metadata": {
        "id": "c86da18d"
      },
      "outputs": [],
      "source": [
        "input_img = np.load(\"./data/globalpooling_input.npy\")[np.newaxis, ...]\n",
        "input_img = input_img.astype('float32')\n",
        "input_img = torch.tensor(input_img).permute(0, 3, 1, 2)\n",
        "input_img.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "402b7417",
      "metadata": {
        "id": "402b7417"
      },
      "source": [
        "* ## Global Average Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5d801e7",
      "metadata": {
        "id": "b5d801e7"
      },
      "source": [
        "![GAP](https://hackmd.io/_uploads/Bk9n-1PIp.gif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ad2608e",
      "metadata": {
        "id": "1ad2608e"
      },
      "outputs": [],
      "source": [
        "print(input_img.shape)\n",
        "print(input_img[0, 0, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f49a8e",
      "metadata": {
        "id": "a4f49a8e"
      },
      "outputs": [],
      "source": [
        "avg_result = nn.AdaptiveAvgPool2d(1)(input_img)\n",
        "avg_result = nn.Flatten()(avg_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "817f4c31",
      "metadata": {
        "id": "817f4c31"
      },
      "outputs": [],
      "source": [
        "print(avg_result.shape)\n",
        "print(avg_result.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ebf868",
      "metadata": {
        "id": "f7ebf868"
      },
      "outputs": [],
      "source": [
        "tensor_operation = input_img.mean((2, 3))\n",
        "print(tensor_operation.shape)\n",
        "print(tensor_operation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4253025e",
      "metadata": {
        "id": "4253025e"
      },
      "source": [
        "[(back...)](#GlobalPooling)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "918a1119",
      "metadata": {
        "id": "918a1119"
      },
      "source": [
        "* ## Global Max Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68986e57",
      "metadata": {
        "id": "68986e57"
      },
      "source": [
        "![GMP](https://hackmd.io/_uploads/B1mJz1DL6.gif)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ef759b",
      "metadata": {
        "id": "83ef759b"
      },
      "outputs": [],
      "source": [
        "max_result = nn.AdaptiveMaxPool2d(1)(input_img)\n",
        "max_result = nn.Flatten()(max_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5759d8f",
      "metadata": {
        "id": "d5759d8f"
      },
      "outputs": [],
      "source": [
        "print(input_img.shape)\n",
        "print(input_img[0, 0, :, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a269f32c",
      "metadata": {
        "id": "a269f32c"
      },
      "outputs": [],
      "source": [
        "print(max_result.shape)\n",
        "print(max_result.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d083809",
      "metadata": {
        "id": "2d083809"
      },
      "source": [
        "[(back...)](#GlobalPooling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6622454",
      "metadata": {
        "id": "c6622454"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}