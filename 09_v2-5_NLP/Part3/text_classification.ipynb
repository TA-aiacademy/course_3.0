{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:  2.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print('tensorflow version: ', tf.__version__)\n",
    "\n",
    "# 指定使用第一張GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 上傳資料\n",
    "# !wget -q https://github.com/TA-aiacademy/course_3.0/releases/download/v2.5_nlp/NLP_part3.zip\n",
    "# !unzip -q NLP_part3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"Data\"\n",
    "zh_vocab_file = os.path.join(output_dir, \"zh_vocab\")\n",
    "checkpoint_path = os.path.join(output_dir, \"checkpoints.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7075, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>反核人士最愛靠妖 核電廠蓋你家好不好，我當然說好 核廢料放我家好不好，我也ok 放在地下室就...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>如標題， 今天去逛才看到的，如下圖所示:  位置在西屯區漢口路二段118號。 少了一個可以看...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>新聞來源  2025非核家園，燃煤電廠30%、再生能源(綠能)20%、 天然氣發電50%的能...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>牽了一台新的摩托車 車行老闆跟我說記得汽油要加95 還附帶 我開車行幾十年了 聽我的準沒錯 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>各位30cm大大、F cup的水水，打給後 胎嘎後 本魯邊緣人，平日臉書沒朋友近日更4鬼怪肆...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  反核人士最愛靠妖 核電廠蓋你家好不好，我當然說好 核廢料放我家好不好，我也ok 放在地下室就...      1\n",
       "1  如標題， 今天去逛才看到的，如下圖所示:  位置在西屯區漢口路二段118號。 少了一個可以看...      1\n",
       "2  新聞來源  2025非核家園，燃煤電廠30%、再生能源(綠能)20%、 天然氣發電50%的能...      1\n",
       "3  牽了一台新的摩托車 車行老闆跟我說記得汽油要加95 還附帶 我開車行幾十年了 聽我的準沒錯 ...      1\n",
       "4  各位30cm大大、F cup的水水，打給後 胎嘎後 本魯邊緣人，平日臉書沒朋友近日更4鬼怪肆...      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptt_gossip = pd.read_csv('Data/ptt_gossip.csv')\n",
    "ptt_gossip.drop(columns='idx', inplace=True)\n",
    "print(ptt_gossip.shape)\n",
    "ptt_gossip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter sentence length\n",
    "\n",
    "依照句子長度過濾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5091, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>如標題， 今天去逛才看到的，如下圖所示:  位置在西屯區漢口路二段118號。 少了一個可以看...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>牽了一台新的摩托車 車行老闆跟我說記得汽油要加95 還附帶 我開車行幾十年了 聽我的準沒錯 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>各位30cm大大、F cup的水水，打給後 胎嘎後 本魯邊緣人，平日臉書沒朋友近日更4鬼怪肆...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>是否有專板 ， 本板並非萬能問板 。 兩則 問卦， 自刪及被刪也算兩篇之內 ， 本看板嚴格禁...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>亞洲盃男籃 中華隊被日本痛宰將近40分 87:49 想寫個慘字</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  如標題， 今天去逛才看到的，如下圖所示:  位置在西屯區漢口路二段118號。 少了一個可以看...      1\n",
       "1  牽了一台新的摩托車 車行老闆跟我說記得汽油要加95 還附帶 我開車行幾十年了 聽我的準沒錯 ...      1\n",
       "2  各位30cm大大、F cup的水水，打給後 胎嘎後 本魯邊緣人，平日臉書沒朋友近日更4鬼怪肆...      1\n",
       "3  是否有專板 ， 本板並非萬能問板 。 兩則 問卦， 自刪及被刪也算兩篇之內 ， 本看板嚴格禁...      1\n",
       "4                   亞洲盃男籃 中華隊被日本痛宰將近40分 87:49 想寫個慘字       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 256\n",
    "\n",
    "ptt_gossip = ptt_gossip[ptt_gossip.sentence.str.len() < max_length]\n",
    "ptt_gossip.reset_index(drop=True, inplace=True)\n",
    "print(ptt_gossip.shape)\n",
    "ptt_gossip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.2\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(ptt_gossip['sentence'],\n",
    "                                                      ptt_gossip['label'],\n",
    "                                                      test_size=valid_size,\n",
    "                                                      shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "1. 將資料轉換成`tf.tensor`格式。\n",
    "2. 使用`tfds.features.text.SubwordTextEncoder`進行斷詞，斷詞方式為`character-level`方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Chinese vocabulary: Data/zh_vocab\n",
      "CPU times: user 23.5 ms, sys: 15.7 ms, total: 39.3 ms\n",
      "Wall time: 29.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    tokenizer_zh = tfds.deprecated.text.SubwordTextEncoder.load_from_file(zh_vocab_file) \n",
    "    print('Load Chinese vocabulary: %s' % zh_vocab_file)\n",
    "except: \n",
    "    print('Build Chinese vocabulary: %s' % zh_vocab_file)\n",
    "    tokenizer_zh = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((x.numpy() for x, y in train_dataset),\n",
    "                                                                             max_subword_length=1,\n",
    "                                                                             target_vocab_size=2**13)\n",
    "    tokenizer_zh.save_to_file(zh_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  4217\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size: ', tokenizer_zh.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SubwordTextEncoder vocab_size=4217>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_zh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence token_id:  [63, 1756, 748, 352, 68, 1581]\n",
      "Tokenization:  ['文', '瑋', '助', '教', '真', '壯']\n"
     ]
    }
   ],
   "source": [
    "sentence = '文瑋助教真壯'\n",
    "token_id = tokenizer_zh.encode(sentence)\n",
    "\n",
    "print('Sentence token_id: ', token_id)\n",
    "print('Tokenization: ', [tokenizer_zh.decode([t]) for t in token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to token_id\n",
    "\n",
    "因為訓練時需要將每個字轉換成，這邊使用`.map`方式將`train_dataset`轉換成`token_id`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sentence, label):\n",
    "    zh_id = tokenizer_zh.encode(sentence.numpy())\n",
    "    return (tf.cast(zh_id, tf.int32), tf.cast(label, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(sentence, label):\n",
    "    \"\"\"\n",
    "    從encode輸出的zh_id不是Eager Tensor\n",
    "    需要透過 tf.py_function 轉為Eager Tensor\n",
    "    \"\"\"\n",
    "    return tf.py_function(encode, [sentence, label], [tf.int32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(tf_encode)\n",
    "valid_dataset = valid_dataset.map(tf_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_valid = next(iter(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(65,), dtype=int32, numpy=\n",
      "array([ 227,  254,  406,   79,  323,  369, 1241, 3993,    3,  127,    1,\n",
      "        405,   12,  227,  254, 3993,   45,  431, 3993,  175,   58,  652,\n",
      "        196, 3993,  281,  385,    8,  749,  134, 3993,  369, 1734, 3993,\n",
      "        196,  483,  345,  258,    6,  272,   53,   10,  254,  151, 3993,\n",
      "        106,   48,  112,  353,  128,    2,  104,  498,   33,    1,  104,\n",
      "        498, 3993,  143,   44,  175,   65,   35,   29,  104,  498],\n",
      "      dtype=int32)>,\n",
      " <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "pprint(tmp_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'情歌王子張信哲 有名的四大情歌 過火 愛如潮水 別怕我傷心 信仰 水準海放一堆現在歌手 請問哪首才是經典中的經典 太想愛你也很經典'\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer_zh.decode(tmp_valid[0].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input pipeline\n",
    "\n",
    "這邊使用`tf.data.Data.from_tensor_slices`建立一個`generator`，每次訓練時讀取`batch_size`張圖片，通常會建立`generator`都是因為圖片量過大無法一次讀入記憶體，這邊使用`generator`是為了示範。\n",
    "\n",
    "1. `.shuffle()`:進行`buffer_size`的打亂，每次從資料中取`buffer_size`個`batch`作為`buffer`，然後再從`buffer`中隨機抽一個`batch`出來做訓練，所以適當的`buffer_size`很重要，如果`buffer_size`過小會導致放在`buffer`裡的都是同一類別的圖片，最好的做法是直接把`buffer_size`設為訓練圖片數量(`len(X_train)`)，這樣能夠確保隨機性。\n",
    "\n",
    "2. `.padded_batch()`:將每個`batch`進行`padding`，符合訓練的輸入格式。\n",
    "\n",
    "3. `.repeat()`: 複製資料集為`epochs`份，訓練時需要`epochs`份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = len(X_train)\n",
    "\n",
    "embedding_size = 256\n",
    "rnn_units = 512\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(buffer_size).padded_batch(batch_size, padded_shapes=([-1], []), drop_remainder=True).repeat(epochs)\n",
    "valid_dataset = valid_dataset.padded_batch(batch_size, padded_shapes=([-1], []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "這邊使用`iter`呼叫`generator`來觀看其中一個`batch`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence.shape:  (64, 243)\n",
      "tf.Tensor(\n",
      "[[ 66 880  79 ...   0   0   0]\n",
      " [ 66 129  40 ...   0   0   0]\n",
      " [ 53  10  51 ...   0   0   0]\n",
      " ...\n",
      " [ 12  30  17 ...   0   0   0]\n",
      " [768 170  12 ...   0   0   0]\n",
      " [ 71  22  14 ...   0   0   0]], shape=(64, 243), dtype=int32)\n",
      "--------------------\n",
      "Label.shape:  (64,)\n",
      "tf.Tensor(\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1], shape=(64,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tmp_generator = iter(train_dataset)\n",
    "tmp_x, tmp_y = next(tmp_generator)\n",
    "\n",
    "print('Sentence.shape: ', tmp_x.shape)\n",
    "print(tmp_x)\n",
    "print('-'*20)\n",
    "print('Label.shape: ', tmp_y.shape)\n",
    "print(tmp_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM model\n",
    "\n",
    "`tensorflow2.0.0`預設是`eager model`，有助於在撰寫模型時`debug`以及觀看數值運算結果。\n",
    "\n",
    "這裡使用`tf.keras`為基底進行建模，在`lstm`中需要注意輸入型態為`(timesteps, feature_size)`，另外常見有三個參數需要注意：\n",
    "\n",
    "1. `embedding_size`: 每個字的詞向量大小。\n",
    "2. `rnn_units`: `lstm`模型的神經元數量。\n",
    "3. `return_sequences`: 是否輸出每個`timestep`的結果(`hidden_state`)，輸出型態為`(batch_size, )`。\n",
    "4. `return_state`: 是否輸出最後一個`timestep`的結果(`hidden_state`和`cell_state`)。\n",
    "\n",
    "其實`3.`和`4.`的功能有點重複了，通常我們只會拿最後一個`timestep`作為輸出，這邊我們將`return_sequences`設為`True`，並使用`slice`方式將最後一個`hidden_sate`拿出來。\n",
    "\n",
    "最後使用`tf.keras.layers.Dense`輸出`2`個類別的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(batch_size, rnn_units):\n",
    "    input_layer = tf.keras.Input(shape=[None],batch_size=batch_size)\n",
    "    embedding_layer = tf.keras.layers.Embedding(tokenizer_zh.vocab_size, embedding_size)(input_layer)\n",
    "    \n",
    "    lstm = tf.keras.layers.LSTM(units=rnn_units,\n",
    "                                activation='tanh',\n",
    "                                recurrent_activation='sigmoid',\n",
    "                                use_bias=True,\n",
    "                                return_sequences=True,\n",
    "                                return_state=False,\n",
    "                                recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    lstm_hidden_states = lstm(embedding_layer)\n",
    "    \n",
    "    lstm_last_state = lstm_hidden_states[:,-1,:]\n",
    "    \n",
    "    output = tf.keras.layers.Dense(2, activation='softmax', name='output')(lstm_last_state)\n",
    "    \n",
    "    return input_layer, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer, output = rnn_model(batch_size,rnn_units)\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(64, None)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (64, None, 256)           1079552   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 512)           1574912   \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (64, 512)                0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " output (Dense)              (64, 2)                   1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,655,490\n",
      "Trainable params: 2,655,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 17:53:41.828388: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 8s 67ms/step - loss: 0.2688 - accuracy: 0.9459 - val_loss: 0.1662 - val_accuracy: 0.9625\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 6s 57ms/step - loss: 0.1729 - accuracy: 0.9601 - val_loss: 0.1613 - val_accuracy: 0.9625\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 6s 58ms/step - loss: 0.1689 - accuracy: 0.9603 - val_loss: 0.1620 - val_accuracy: 0.9625\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 6s 57ms/step - loss: 0.1701 - accuracy: 0.9601 - val_loss: 0.1604 - val_accuracy: 0.9625\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 6s 58ms/step - loss: 0.1693 - accuracy: 0.9598 - val_loss: 0.1610 - val_accuracy: 0.9625\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 6s 58ms/step - loss: 0.1698 - accuracy: 0.9598 - val_loss: 0.1620 - val_accuracy: 0.9625\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 6s 58ms/step - loss: 0.1699 - accuracy: 0.9601 - val_loss: 0.1649 - val_accuracy: 0.9625\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 6s 58ms/step - loss: 0.1680 - accuracy: 0.9603 - val_loss: 0.1600 - val_accuracy: 0.9625\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 6s 58ms/step - loss: 0.1692 - accuracy: 0.9598 - val_loss: 0.1601 - val_accuracy: 0.9625\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 6s 58ms/step - loss: 0.1699 - accuracy: 0.9596 - val_loss: 0.1604 - val_accuracy: 0.9625\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=len(X_train) // batch_size,\n",
    "                    validation_data=valid_dataset,\n",
    "                    validation_steps=len(X_valid) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing prediction\n",
    "\n",
    "觀察`testing`的`precision, recall, f1-score`以及`confusion matrix`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred = model.predict(valid_dataset)\n",
    "valid_pred_id = np.argmax(valid_pred, axis=-1)\n",
    "valid_true_id = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        40\n",
      "           1       0.96      1.00      0.98       979\n",
      "\n",
      "    accuracy                           0.96      1019\n",
      "   macro avg       0.48      0.50      0.49      1019\n",
      "weighted avg       0.92      0.96      0.94      1019\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred = valid_pred_id, y_true = valid_true_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_0</th>\n",
       "      <th>Pred_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual_0</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_1</th>\n",
       "      <td>0</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pred_0  Pred_1\n",
       "Actual_0       0      40\n",
       "Actual_1       0     979"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confm = confusion_matrix(y_pred = valid_pred_id, y_true = valid_true_id)\n",
    "pd.DataFrame(confm, index=['Actual_0', 'Actual_1'], columns=['Pred_0', 'Pred_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
